{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 110913/110913 [00:00<00:00, 682894.62it/s]\n",
      "4000it [00:05, 755.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total images : 4000\n"
     ]
    }
   ],
   "source": [
    "import detectron2\n",
    "from detectron2.utils.logger import setup_logger\n",
    "\n",
    "# import some common libraries\n",
    "import numpy as np\n",
    "import cv2\n",
    "import random\n",
    "#from google.colab.patches import cv2_imshow\n",
    "\n",
    "# import some common detectron2 utilities\n",
    "from detectron2 import model_zoo\n",
    "from detectron2.engine import DefaultPredictor\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.utils.visualizer import Visualizer\n",
    "from detectron2.data import MetadataCatalog\n",
    "import torch\n",
    "\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as transforms\n",
    "from matplotlib.pyplot import imshow\n",
    "from PIL import Image\n",
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "from detectron2.structures import BoxMode\n",
    "\n",
    "from detectron2.config import get_cfg\n",
    "\n",
    "from detectron2.data import DatasetCatalog, MetadataCatalog\n",
    "\n",
    "from detectron2.modeling import build_model\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.autograd import Variable\n",
    "import csv \n",
    "from defaults import DefaultBatchPredictor\n",
    "######################################################################################\n",
    "import argparse\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--data_dir', type=str, default=None)\n",
    "parser.add_argument('--model_dir', type=str, default=None)\n",
    "\n",
    "parser.add_argument('--batch', type=int, default=24)\n",
    "\n",
    "parser.add_argument('--num_of_data', type=int, default=42000)\n",
    "parser.add_argument('--data_random', type=bool, default=False)\n",
    "\n",
    "parser.add_argument('-f')\n",
    "args = parser.parse_args()\n",
    "\n",
    "######################################################################################\n",
    "\n",
    "data_path = '/root/notebooks/final/caltech_origin_data_refine/'\n",
    "model_path = '/root/notebooks/final/detectron2_out_part3_25p/'\n",
    "\n",
    "num_of_data = 4000\n",
    "\n",
    "data_random_select = args.data_random\n",
    "\n",
    "batch = 5\n",
    "\n",
    "def get_pedestrain_dict(image_list, json_list):\n",
    "    dataset_dicts = []\n",
    "    \n",
    "    for i,path in tqdm(enumerate(image_list)):\n",
    "        filename = path\n",
    "        #img = cv2.imread(path)\n",
    "        # height, width = cv2.imread(filename).shape[:2]\n",
    "        record = {}\n",
    "        record['file_name'] = filename\n",
    "        #record['file_img'] = img\n",
    "        record['image_id'] = i #path.split('/')[-1][:-5]\n",
    "        #id is like 000000 or 000001\n",
    "        record['height']= 480\n",
    "        record['width']= 640\n",
    "        \n",
    "        #for i in data_list[1] to get bbox and category\n",
    "        objs = []\n",
    "        \n",
    "        people = json_list[i]\n",
    "        with open(people) as p:\n",
    "            json_context = json.load(p)\n",
    "            for person in json_context:\n",
    "                boxes = list(map(float, person['pos']))\n",
    "                obj = {\n",
    "                    \"bbox\": boxes,\n",
    "                    \"bbox_mode\": BoxMode.XYWH_ABS,\n",
    "                    #\"segmentation\": [poly], To draw a line, along to ballon\n",
    "                    \"category_id\": 0,\n",
    "                    \"iscrowd\": 0\n",
    "                }\n",
    "                objs.append(obj)\n",
    "            record[\"annotations\"] = objs\n",
    "        dataset_dicts.append(record)\n",
    "    return dataset_dicts #list of dicts\n",
    "\n",
    "\n",
    "def take_path(path):\n",
    "    tmp = []\n",
    "    for json_number in tqdm(sorted(os.listdir(path))):\n",
    "        if json_number == '.ipynb_checkpoints':\n",
    "            pass\n",
    "        else: tmp.append(os.path.join(path, json_number))\n",
    "    return tmp\n",
    "\n",
    "\n",
    "data_pathlist_json = take_path(data_path+'street_json/')\n",
    "\n",
    "data_pathlist_json = data_pathlist_json[42000:(42000+num_of_data)]\n",
    "\n",
    "train_people_jsonlist = data_pathlist_json\n",
    "random.shuffle(train_people_jsonlist)\n",
    "train_people_imagelist = [i.replace('street_json', 'street').replace('/json/', '/output/').replace('.json', '.jpg') for i in train_people_jsonlist]\n",
    "\n",
    "dataset_dicts = get_pedestrain_dict(train_people_imagelist, train_people_jsonlist)\n",
    "\n",
    "#print(dataset_dicts)\n",
    "print(\"Total images : \"+str(len(train_people_imagelist)))\n",
    "\n",
    "\n",
    "cfg = get_cfg()\n",
    "\n",
    "cfg.merge_from_file(model_zoo.get_config_file(\"Base-RCNN-FPN.yaml\"))\n",
    "cfg.DATALOADER.NUM_WORKERS = 4\n",
    "cfg.SOLVER.IMS_PER_BATCH = 3\n",
    "cfg.SOLVER.BASE_LR = 0.00025\n",
    "cfg.SOLVER.MAX_ITER =  50000\n",
    "cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 512\n",
    "cfg.MODEL.ROI_HEADS.NUM_CLASSES = 1 \n",
    "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.7\n",
    "\n",
    "cfg.MODEL.WEIGHTS = os.path.join('/root/notebooks/final/', \"model_final.pth\")\n",
    "\n",
    "#pre = DefaultPredictor(cfg)\n",
    "                            \n",
    "#loader = pre.create_batch_loader(train_people_imagelist, train_people_jsonlist, batch)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result, result_dic = pre.batchPredictor()\n",
    "    \n",
    "# print(result[1])\n",
    "# print(result_dic[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre2 = DefaultPredictor(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "iou = 0.5\n",
    "def compute_TF(outputs, d, iou=0.5):\n",
    "    TF_list = []\n",
    "    for box in outputs['instances'].pred_boxes.to(\"cpu\"):\n",
    "        \n",
    "        for annotation in d['annotations']:\n",
    "            curr_iou = 0\n",
    "            flag = False\n",
    "            \n",
    "            top, left = annotation['bbox'][0], annotation['bbox'][1]\n",
    "            bottom, right = top+annotation['bbox'][2], left+annotation['bbox'][3] \n",
    "            \n",
    "            curr_iou = compute_iou((top, left, bottom, right), box)\n",
    "            \n",
    "            if curr_iou >= iou:\n",
    "                TF_list.append('T')\n",
    "                flag = True\n",
    "                break\n",
    "        if flag == False:\n",
    "            TF_list.append('F')\n",
    "    return TF_list\n",
    "\n",
    "def compute_iou(rec1, rec2):\n",
    "    \"\"\"\n",
    "    computing IoU\n",
    "    :param rec1: (y0, x0, y1, x1), which reflects\n",
    "            (top, left, bottom, right)\n",
    "    :param rec2: (y0, x0, y1, x1)\n",
    "    :return: scala value of IoU\n",
    "    \"\"\"\n",
    "    # computing area of each rectangles\n",
    "    S_rec1 = (rec1[2] - rec1[0]) * (rec1[3] - rec1[1])\n",
    "    S_rec2 = (rec2[2] - rec2[0]) * (rec2[3] - rec2[1])\n",
    " \n",
    "    # computing the sum_area\n",
    "    sum_area = S_rec1 + S_rec2\n",
    " \n",
    "    # find the each edge of intersect rectangle\n",
    "    left_line = max(rec1[1], rec2[1])\n",
    "    right_line = min(rec1[3], rec2[3])\n",
    "    top_line = max(rec1[0], rec2[0])\n",
    "    bottom_line = min(rec1[2], rec2[2])\n",
    " \n",
    "    # judge if there is an intersect\n",
    "    if left_line >= right_line or top_line >= bottom_line:\n",
    "        return 0\n",
    "    else:\n",
    "        intersect = (right_line - left_line) * (bottom_line - top_line)\n",
    "        return (intersect / (sum_area - intersect))*1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = dataset_dicts\n",
    "\n",
    "x = []\n",
    "y = []\n",
    "\n",
    "MR_list = []\n",
    "FP_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(d[0]['annotations'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(200):\n",
    "    \n",
    "    outputs = pre2(cv2.imread(train_people_imagelist[i]))\n",
    "    \n",
    "    d = dataset_dicts[i]\n",
    "    \n",
    "    TP = 0\n",
    "    \n",
    "    TF_list = compute_TF(outputs, d, iou)\n",
    "    for TF in TF_list:\n",
    "        if TF == 'T':\n",
    "            TP += 1\n",
    "    if len(d['annotations']) > TP:\n",
    "        MR = (len(d['annotations']) - TP) / len(d['annotations'])\n",
    "        FP = len(outputs['instances']) - TP\n",
    "    else: \n",
    "        MR = 0\n",
    "        FP = len(outputs['instances']) - len(d['annotations'])\n",
    "\n",
    "    MR_list.append(MR)\n",
    "    FP_list.append(FP)\n",
    "    #break\n",
    "        \n",
    "x.append(sum(FP_list)/len(FP_list))\n",
    "y.append(sum(MR_list)/len(MR_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.31] [0.8885694444444446]\n"
     ]
    }
   ],
   "source": [
    "print(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = get_cfg()\n",
    "\n",
    "cfg.merge_from_file(model_zoo.get_config_file(\"Base-RCNN-FPN.yaml\"))\n",
    "cfg.DATALOADER.NUM_WORKERS = 4\n",
    "cfg.SOLVER.IMS_PER_BATCH = 3\n",
    "cfg.SOLVER.BASE_LR = 0.00025\n",
    "cfg.SOLVER.MAX_ITER =  50000\n",
    "cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 512\n",
    "cfg.MODEL.ROI_HEADS.NUM_CLASSES = 1 \n",
    "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.7\n",
    "\n",
    "cfg.MODEL.WEIGHTS = os.path.join('/root/notebooks/final/', \"model_final.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold test :  0.3  ( 1 / 13 )\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-65-31758478330f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMODEL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mROI_HEADS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSCORE_THRESH_TEST\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m100.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mpre2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDefaultBatchPredictor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mMR_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/final/pedestrian_generator/Detectron2/defaults.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, cfg, togpu, modelPath)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmodelPath\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m             \u001b[0mcheckpointer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMODEL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWEIGHTS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Load weight from : \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMODEL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWEIGHTS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/fvcore/common/checkpoint.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self, path, checkpointables)\u001b[0m\n\u001b[1;32m    115\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Checkpoint {} not found!\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m         \u001b[0mcheckpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0mincompatible\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         if (\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/detectron2/checkpoint/detection_checkpoint.py\u001b[0m in \u001b[0;36m_load_file\u001b[0;34m(self, filename)\u001b[0m\n\u001b[1;32m     40\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"model\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"__author__\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"Caffe2\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"matching_heuristics\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0mloaded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# load native pth checkpoint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"model\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloaded\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0mloaded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"model\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mloaded\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/fvcore/common/checkpoint.py\u001b[0m in \u001b[0;36m_load_file\u001b[0;34m(self, f)\u001b[0m\n\u001b[1;32m    211\u001b[0m                 \u001b[0mto\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m         \"\"\"\n\u001b[0;32m--> 213\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_load_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheckpoint\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0m_IncompatibleKeys\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pyre-ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    527\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0m_open_zipfile_reader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    528\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0m_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_zipfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 529\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_legacy_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    530\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_legacy_load\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    707\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdeserialized_storage_keys\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    708\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdeserialized_objects\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 709\u001b[0;31m         \u001b[0mdeserialized_objects\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_from_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moffset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf_should_read_directly\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    710\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0moffset\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    711\u001b[0m             \u001b[0moffset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "resu = []\n",
    "x = []\n",
    "y = []\n",
    "\n",
    "count = 1\n",
    "\n",
    "gap = 5\n",
    "for threshold in range(30,105,gap):\n",
    "    print(\"Threshold test : \",threshold/100.0,\" (\" ,count,'/',len(range(40,105,gap)), \")\")\n",
    "    count+=1\n",
    "#     x,y = compuete_MR(threshold/100.0)\n",
    "    cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = threshold/100.0\n",
    "    \n",
    "    pre2 = DefaultBatchPredictor(cfg)\n",
    "    \n",
    "    MR_list = []\n",
    "    FP_list = []\n",
    "    \n",
    "    for i in range(100):\n",
    "    \n",
    "        outputs = pre2(cv2.imread(train_people_imagelist[i]))\n",
    "\n",
    "        d = dataset_dicts[i]\n",
    "\n",
    "        TP = 0\n",
    "\n",
    "        TF_list = compute_TF(outputs, d, iou)\n",
    "        for TF in TF_list:\n",
    "            if TF == 'T':\n",
    "                TP += 1\n",
    "        if len(d['annotations']) > TP:\n",
    "            MR = (len(d['annotations']) - TP) / len(d['annotations'])\n",
    "            FP = len(outputs['instances']) - TP\n",
    "        else: \n",
    "            MR = 0\n",
    "            FP = len(outputs['instances']) - len(d['annotations'])\n",
    "\n",
    "        MR_list.append(MR)\n",
    "        FP_list.append(FP)\n",
    "    x = sum(FP_list)/len(FP_list)\n",
    "    y = sum(MR_list)/len(MR_list)\n",
    "    resu.append([x,y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'resu' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-a7db8e646190>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'resu' is not defined"
     ]
    }
   ],
   "source": [
    "print(resu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join('/root/notebooks/final/', \"model_result_D.csv\") , 'w+', newline ='') as f:\n",
    "    write = csv.writer(f) \n",
    "    write.writerows(resu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python plot.py --csv_path '/root/notebooks/final/model_result_D.csv' --output_dir '/root/notebooks/final/'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image.open('/root/notebooks/final/plot.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load weight from :  /root/notebooks/final/model_final.pth\n",
      "{'instances': Instances(num_instances=2, image_height=480, image_width=640, fields=[pred_boxes: Boxes(tensor([[342.4135, 190.9354, 376.8608, 262.4553],\n",
      "        [231.9504, 174.6648, 278.0927, 275.0215]], device='cuda:0')), scores: tensor([0.4861, 0.3432], device='cuda:0'), pred_classes: tensor([0, 0], device='cuda:0')])}\n",
      "{'instances': Instances(num_instances=2, image_height=480, image_width=640, fields=[pred_boxes: Boxes(tensor([[152.7999, 151.3908, 179.3864, 208.8824],\n",
      "        [459.0749, 207.8614, 513.2574, 320.2764]], device='cuda:0')), scores: tensor([0.7124, 0.3711], device='cuda:0'), pred_classes: tensor([0, 0], device='cuda:0')])}\n",
      "{'instances': Instances(num_instances=4, image_height=480, image_width=640, fields=[pred_boxes: Boxes(tensor([[599.9340, 197.8654, 640.0000, 231.0118],\n",
      "        [556.5503, 159.6331, 586.8049, 223.9113],\n",
      "        [140.0022, 184.6031, 169.6389, 248.7350],\n",
      "        [  0.7024, 186.7161,  39.8138, 227.6464]], device='cuda:0')), scores: tensor([0.6158, 0.6145, 0.4560, 0.3143], device='cuda:0'), pred_classes: tensor([0, 0, 0, 0], device='cuda:0')])}\n"
     ]
    }
   ],
   "source": [
    "pre2 = DefaultBatchPredictor(cfg)\n",
    "\n",
    "for i in range(3):\n",
    "    outputs = pre2(cv2.imread(train_people_imagelist[i]))\n",
    "    print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load weight from :  /root/notebooks/final/model_final.pth\n"
     ]
    }
   ],
   "source": [
    "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.95\n",
    "\n",
    "pre2 = DefaultBatchPredictor(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20it [00:00, 587.49it/s]\n"
     ]
    }
   ],
   "source": [
    "_ = pre2.create_batch_loader(train_people_imagelist[:20], train_people_jsonlist[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.85s/it]\n"
     ]
    }
   ],
   "source": [
    "r1, r2 = pre2.batchPredictor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'instances': Instances(num_instances=0, image_height=480, image_width=640, fields=[pred_boxes: Boxes(tensor([], device='cuda:0', size=(0, 4))), scores: tensor([], device='cuda:0'), pred_classes: tensor([], device='cuda:0', dtype=torch.int64)])},\n",
       " {'instances': Instances(num_instances=0, image_height=480, image_width=640, fields=[pred_boxes: Boxes(tensor([], device='cuda:0', size=(0, 4))), scores: tensor([], device='cuda:0'), pred_classes: tensor([], device='cuda:0', dtype=torch.int64)])},\n",
       " {'instances': Instances(num_instances=0, image_height=480, image_width=640, fields=[pred_boxes: Boxes(tensor([], device='cuda:0', size=(0, 4))), scores: tensor([], device='cuda:0'), pred_classes: tensor([], device='cuda:0', dtype=torch.int64)])},\n",
       " {'instances': Instances(num_instances=0, image_height=480, image_width=640, fields=[pred_boxes: Boxes(tensor([], device='cuda:0', size=(0, 4))), scores: tensor([], device='cuda:0'), pred_classes: tensor([], device='cuda:0', dtype=torch.int64)])},\n",
       " {'instances': Instances(num_instances=0, image_height=480, image_width=640, fields=[pred_boxes: Boxes(tensor([], device='cuda:0', size=(0, 4))), scores: tensor([], device='cuda:0'), pred_classes: tensor([], device='cuda:0', dtype=torch.int64)])},\n",
       " {'instances': Instances(num_instances=0, image_height=480, image_width=640, fields=[pred_boxes: Boxes(tensor([], device='cuda:0', size=(0, 4))), scores: tensor([], device='cuda:0'), pred_classes: tensor([], device='cuda:0', dtype=torch.int64)])},\n",
       " {'instances': Instances(num_instances=0, image_height=480, image_width=640, fields=[pred_boxes: Boxes(tensor([], device='cuda:0', size=(0, 4))), scores: tensor([], device='cuda:0'), pred_classes: tensor([], device='cuda:0', dtype=torch.int64)])},\n",
       " {'instances': Instances(num_instances=0, image_height=480, image_width=640, fields=[pred_boxes: Boxes(tensor([], device='cuda:0', size=(0, 4))), scores: tensor([], device='cuda:0'), pred_classes: tensor([], device='cuda:0', dtype=torch.int64)])},\n",
       " {'instances': Instances(num_instances=0, image_height=480, image_width=640, fields=[pred_boxes: Boxes(tensor([], device='cuda:0', size=(0, 4))), scores: tensor([], device='cuda:0'), pred_classes: tensor([], device='cuda:0', dtype=torch.int64)])},\n",
       " {'instances': Instances(num_instances=0, image_height=480, image_width=640, fields=[pred_boxes: Boxes(tensor([], device='cuda:0', size=(0, 4))), scores: tensor([], device='cuda:0'), pred_classes: tensor([], device='cuda:0', dtype=torch.int64)])},\n",
       " {'instances': Instances(num_instances=0, image_height=480, image_width=640, fields=[pred_boxes: Boxes(tensor([], device='cuda:0', size=(0, 4))), scores: tensor([], device='cuda:0'), pred_classes: tensor([], device='cuda:0', dtype=torch.int64)])},\n",
       " {'instances': Instances(num_instances=0, image_height=480, image_width=640, fields=[pred_boxes: Boxes(tensor([], device='cuda:0', size=(0, 4))), scores: tensor([], device='cuda:0'), pred_classes: tensor([], device='cuda:0', dtype=torch.int64)])},\n",
       " {'instances': Instances(num_instances=0, image_height=480, image_width=640, fields=[pred_boxes: Boxes(tensor([], device='cuda:0', size=(0, 4))), scores: tensor([], device='cuda:0'), pred_classes: tensor([], device='cuda:0', dtype=torch.int64)])},\n",
       " {'instances': Instances(num_instances=1, image_height=480, image_width=640, fields=[pred_boxes: Boxes(tensor([[194.1030, 172.3160, 279.6630, 349.6762]], device='cuda:0')), scores: tensor([0.9573], device='cuda:0'), pred_classes: tensor([0], device='cuda:0')])},\n",
       " {'instances': Instances(num_instances=1, image_height=480, image_width=640, fields=[pred_boxes: Boxes(tensor([[494.8411, 149.8825, 512.0273, 199.9344]], device='cuda:0')), scores: tensor([0.9757], device='cuda:0'), pred_classes: tensor([0], device='cuda:0')])},\n",
       " {'instances': Instances(num_instances=0, image_height=480, image_width=640, fields=[pred_boxes: Boxes(tensor([], device='cuda:0', size=(0, 4))), scores: tensor([], device='cuda:0'), pred_classes: tensor([], device='cuda:0', dtype=torch.int64)])},\n",
       " {'instances': Instances(num_instances=0, image_height=480, image_width=640, fields=[pred_boxes: Boxes(tensor([], device='cuda:0', size=(0, 4))), scores: tensor([], device='cuda:0'), pred_classes: tensor([], device='cuda:0', dtype=torch.int64)])},\n",
       " {'instances': Instances(num_instances=2, image_height=480, image_width=640, fields=[pred_boxes: Boxes(tensor([[316.1447, 149.8755, 357.4860, 247.9883],\n",
       "          [  3.0631, 178.3573,  21.6027, 245.5290]], device='cuda:0')), scores: tensor([0.9746, 0.9536], device='cuda:0'), pred_classes: tensor([0, 0], device='cuda:0')])},\n",
       " {'instances': Instances(num_instances=0, image_height=480, image_width=640, fields=[pred_boxes: Boxes(tensor([], device='cuda:0', size=(0, 4))), scores: tensor([], device='cuda:0'), pred_classes: tensor([], device='cuda:0', dtype=torch.int64)])},\n",
       " {'instances': Instances(num_instances=0, image_height=480, image_width=640, fields=[pred_boxes: Boxes(tensor([], device='cuda:0', size=(0, 4))), scores: tensor([], device='cuda:0'), pred_classes: tensor([], device='cuda:0', dtype=torch.int64)])}]"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "MR_list = []\n",
    "FP_list = []\n",
    "\n",
    "for i in range(len(r1)):\n",
    "    outputs = r1[i]\n",
    "    d = dataset_dicts[i]\n",
    "    \n",
    "    TP = 0\n",
    "\n",
    "    TF_list = compute_TF(outputs, d, iou)\n",
    "    for TF in TF_list:\n",
    "        if TF == 'T':\n",
    "            TP += 1\n",
    "    if len(d['annotations']) > TP:\n",
    "        MR = (len(d['annotations']) - TP) / len(d['annotations'])\n",
    "        FP = len(outputs['instances']) - TP\n",
    "    else: \n",
    "        MR = 0\n",
    "        FP = len(outputs['instances']) - len(d['annotations'])\n",
    "\n",
    "    MR_list.append(MR)\n",
    "    FP_list.append(FP)\n",
    "x = sum(FP_list)/len(FP_list)\n",
    "y = sum(MR_list)/len(MR_list)\n",
    "resu.append([x,y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[3.25, 0.6658333333333334],\n",
       " [0.75, 0.8408333333333335],\n",
       " [0.3, 0.8758333333333332],\n",
       " [0.2, 0.9316666666666666],\n",
       " [0.0, 1.0],\n",
       " [0.05, 0.9541666666666666]]"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_XY(r, r_dic):\n",
    "    MR_list =[]\n",
    "    FP_list = []\n",
    "    \n",
    "    TP = 0\n",
    "    for i in range(len(r)):\n",
    "        outputs = r[i]\n",
    "        d = dataset_dicts[i]\n",
    "\n",
    "        TP = 0\n",
    "\n",
    "        TF_list = compute_TF(outputs, d, iou)\n",
    "        for TF in TF_list:\n",
    "            if TF == 'T':\n",
    "                TP += 1\n",
    "        if len(d['annotations']) > TP:\n",
    "            MR = (len(d['annotations']) - TP) / len(d['annotations'])\n",
    "            FP = len(outputs['instances']) - TP\n",
    "        else: \n",
    "            MR = 0\n",
    "            FP = len(outputs['instances']) - len(d['annotations'])\n",
    "\n",
    "        MR_list.append(MR)\n",
    "        FP_list.append(FP)\n",
    "    x = sum(FP_list)/len(FP_list)\n",
    "    y = sum(MR_list)/len(MR_list)\n",
    "    print('X:',x)\n",
    "    print('Y:',y)\n",
    "    return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.05, 0.9541666666666666)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "com(r1,dataset_dicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
